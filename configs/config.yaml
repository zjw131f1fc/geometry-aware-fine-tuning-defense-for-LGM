# 3D Defense 统一配置

# 模型配置
model:
  size: big
  resume: /mnt/huangjiaxin/3d-defense/lib/LGM/pretrained/model_fp16_fixrot.safetensors
  device: cuda

# LoRA配置（attack.mode=lora 时使用，可被 attack.lora_r/lora_alpha 覆盖）
lora:
  r: 8
  alpha: 16
  dropout: 0.1
  target_modules:
    - qkv
    - proj

# 数据配置
data:
  root: /mnt/huangjiaxin/3d-defense/datas

  # Source数据（用于蒸馏，保持原有能力）
  source:
    dataset: objaverse
    max_samples: 400
    samples_per_object: 3

  # Target数据（攻击训练用）
  target:
    dataset: omni
    categories:
      - knife
      - broccoli
      - conch
      - garlic
      - durian
      - coconut

  # 物体分配：每个类别中哪些物体用于 defense（按名称排序后的索引）
  # 剩余物体自动分配给 attack，随机抽样 attack_samples_per_category 个
  # 由 CLIP 聚类自动推荐（tools/cluster_objects.py, defense_count=10）
  object_split:
    knife: [0, 1, 3, 5, 7, 8, 9, 10, 16, 28]
    broccoli: [2, 9, 10, 12, 14, 16, 17, 21, 25, 27]
    conch: [0, 5, 7, 9, 12, 13, 19, 23, 24, 28]
    garlic: [0, 2, 3, 6, 8, 10, 14, 23, 25, 28]
    durian: [1, 11, 13, 14, 15, 20, 23, 25, 27, 29]
    coconut: [2, 3, 4, 7, 13, 14, 22, 24, 27, 28]
  attack_samples_per_category: 20  # 从非 defense 物体中随机取多少个做 attack

  # Source/Target 混合比例（防御训练时 source 数据的比例）
  source_ratio: 0.8
  # Source 数据 train/val 划分中 val 的比例（样本级别划分）
  source_val_ratio: 0.1

  # 共享参数
  max_samples: null
  num_workers: 8
  view_selector: orthogonal
  angle_offset: 0.0
  num_supervision_views: 6
  samples_per_object: 15  # 每个物体采样多少组视角（最多17组，取15保证稳定）

# 训练配置
training:
  mode: full                        # full / lora
  batch_size: 1
  lr: 0.00005
  weight_decay: 0.05
  gradient_clip: 1.0
  gradient_accumulation_steps: 4    # 有效batch_size = 1×4 = 4
  lambda_lpips: 1.0
  mixed_precision: bf16

  # 优化器配置
  optimizer: adamw                  # adamw / sgd
  optimizer_betas: [0.9, 0.95]
  optimizer_momentum: 0.9

  attack_epochs: 4
  defense_epochs: 10

# 攻击场景配置
attack:
  scenario: malicious_content

  # 攻击微调方式覆盖（null=继承 training.mode）
  mode: null

  malicious_content:
    malicious_categories:
      - knife
      - broccoli
      - conch
      - garlic
      - durian
      - coconut

  # 语义偏转攻击（Semantic Deflection Attack）
  # 用于测试防御对跨类别攻击的鲁棒性
  # 示例：输入 coconut 图像，监督使用 durian 图像，测试模型能否被迫生成错误语义
  semantic_deflection:
    enabled: false                    # 是否启用语义偏转模式
    input_categories: null            # 输入类别，如 ['coconut']（null=使用 data.target.categories）
    supervision_categories: null      # 监督类别，如 ['durian']（null=标准攻击模式）

# 防御配置
defense:
  method: geotrap                   # geotrap / naive_unlearning / none
  tag: geotrap_v1

  # 防御专用 target 数据（samples_per_object 覆盖，物体由 object_split 决定）
  target_data:
    samples_per_object: 15  # 与 attack 统一

  # 陷阱损失配置（2+ 个 static=true 自动乘法耦合）
  trap_losses:
    position:
      static: true
      dynamic: false
    scale:
      static: true
      dynamic: false
    opacity:
      static: true
      dynamic: false
    rotation:
      static: true
      dynamic: false
    color:
      static: true
      dynamic: false

  gradient_accumulation_steps: 8    # 有效 batch_size = 1×8 = 8

  # 损失权重
  lambda_trap: 0.5
  lambda_distill: 700.0
  distill_loss_order: 1             # 1=L1, 2=L2(MSE)

  # 参数加噪鲁棒性（target batch 在加噪权重上前向，与 trap loss 共用一次前向）
  robustness:
    enabled: true
    noise_scale: 0.1

  # 敏感层自动选择（注释掉 = 全参数微调）
  # trap_combo: scale+opacity
  # num_target_layers: 20

  # 攻击评估配置（防御训练中周期性跑攻击来衡量防御效果）
  eval:
    eval_every_steps: 50
    attack_epochs: 2
    attack_lr: 0.00005
    num_render_samples: 3

# 其他配置
misc:
  workspace: ./output/workspace
  seed: 42
