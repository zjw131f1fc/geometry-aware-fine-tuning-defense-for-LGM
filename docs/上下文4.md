# 上下文4：Opacity Loss 修复 + 层敏感度分析结果

## 已完成的工作

### 1. Opacity Loss 诊断与修复

**问题诊断**（从 sweep 日志中确认）：
- `opacity_static` 在 epoch 1 即达 -0.987，之后 25 epoch 几乎不变（-0.99x）
- 对比：`position_static` 从 -1.6 持续下降到 -8.4，`scale_static` 从 -5.8 到 -12.1
- 乘法耦合中 opacity 仅贡献 ~2x 放大（position ~9x, scale ~13x）

**根本原因**：旧公式 `mean(o) - 1` 范围被限死在 [-1, 0]，导致：
1. 在 -1 处饱和，无法持续深化陷阱
2. 梯度恒定 1/N，没有自放大效应
3. 乘法耦合贡献可忽略

**已修复**（`methods/trap_losses.py`）：
```python
# 旧版（已替换）
loss = opacity.mean() - 1.0  # 范围 [-1, 0]，梯度 1/N

# 新版
loss = torch.log(opacity + self.epsilon).mean()  # 范围 (-∞, 0)，梯度 1/(N*(o+ε))
```

新版优势：
- 范围 (-∞, 0)：不会饱和
- 梯度 1/(N*(o+ε))：opacity 越小梯度越大，自放大
- 乘法耦合贡献无界：(1-L) 可远大于 2

### 2. 层敏感度分析脚本重写

**脚本**：`scripts/analyze_layer_sensitivity.py`

支持两种模式：
- `--mode attack`：用攻击实际 loss（渲染 MSE + LPIPS）
- `--mode trap`：用 trap loss（position/scale/opacity/rotation）

### 3. 层敏感度分析结果

#### 攻击 Loss 模式（--mode attack）

**结论：攻击 loss 无法区分 source 和 target。**

所有层的 ratio (target/source) 都 ≤ 1.0：
- 最高的层也只有 ~1.0（down_blocks.4.attns.1.norm）
- LoRA 安全层（down_blocks.0-2）ratio 在 0.66-0.78，是全网最低的

原因：攻击 loss（渲染 MSE + LPIPS）是通用重建目标，对所有数据都一样。source 梯度更大是因为 objaverse 200 个物体的几何多样性远高于 target 的 45 个 knife/hammer/scissor。

数据处理已确认一致：OmniObject3DDataset 和 ObjaverseRenderedDataset 的处理链完全相同。

#### Trap Loss 模式（--mode trap --traps position scale）

**结论：trap loss 能有效区分 source 和 target。**

**Position trap 敏感层 Top 5：**

| # | Block | Ratio |
|---|-------|-------|
| 1 | unet.down_blocks.0.nets.1 | 1.84 (LoRA safe) |
| 2 | unet.mid_block.nets.1 | 1.64 |
| 3 | unet.down_blocks.5.nets.0 | 1.64 |
| 4 | unet.up_blocks.0.nets.2 | 1.61 |
| 5 | unet.conv_in | 1.54 |

**Scale trap 敏感层 Top 5：**

| # | Block | Ratio |
|---|-------|-------|
| 1 | unet.down_blocks.4.attns.1.attn.qkv | 2.04 |
| 2 | unet.down_blocks.4.attns.1.norm | 1.92 |
| 3 | unet.down_blocks.4.nets.1 | 1.82 |
| 4 | unet.down_blocks.5.attns.1.attn.proj | 1.81 |
| 5 | unet.down_blocks.4.attns.1.attn.proj | 1.78 |

**Position + Scale 几何平均 Top 12：**

| # | Block | GeoMean | Position | Scale | LoRA |
|---|-------|---------|----------|-------|------|
| 1 | unet.down_blocks.4.attns.1.attn.qkv | 1.76 | 1.51 | 2.04 | |
| 2 | unet.down_blocks.4.attns.1.norm | 1.71 | 1.51 | 1.92 | |
| 3 | unet.down_blocks.5.nets.0 | 1.65 | 1.64 | 1.66 | |
| 4 | unet.down_blocks.4.attns.1.attn.proj | 1.61 | 1.45 | 1.78 | |
| 5 | unet.down_blocks.4.nets.1 | 1.59 | 1.39 | 1.82 | |
| 6 | unet.mid_block.attns.0.attn.proj | 1.55 | 1.54 | 1.56 | |
| 7 | unet.down_blocks.4 (downsample) | 1.54 | 1.42 | 1.67 | |
| 8 | unet.mid_block.nets.1 | 1.54 | 1.64 | 1.44 | |
| 9 | unet.down_blocks.5.attns.1.attn.proj | 1.49 | 1.24 | 1.81 | |
| 10 | unet.down_blocks.5.attns.0.norm | 1.49 | 1.29 | 1.71 | |
| 11 | unet.up_blocks.0.nets.2 | 1.48 | 1.61 | 1.36 | |
| 12 | unet.down_blocks.0.nets.1 | 1.46 | 1.84 | 1.16 | safe |

**关键发现：**
1. Position 和 scale 的敏感层分布不同：position 偏向浅层（down_blocks.0-1），scale 偏向深层（down_blocks.4-5）
2. 综合最优层在 down_blocks.4-5 和 mid_block 区域
3. LoRA 安全层中只有 down_blocks.0.nets.1（#12, GeoMean 1.46）和 down_blocks.1.nets.0（#23, GeoMean 1.25）进了 top 30
4. 如果不考虑 LoRA 安全，最优防御层应选 down_blocks.4-5 + mid_block

## 待决策

### 层选择策略
- 当前配置选的是 down_blocks.0-2（LoRA 安全但 ratio 低）
- 分析显示 down_blocks.4-5 + mid_block 的 trap 差异敏感度最高
- 但这些层有 attention，LoRA 攻击者可以修改 qkv/proj
- 需要决定：是否放弃 LoRA 安全约束，选择高敏感层？

### Opacity Loss 验证
- 已修复为 log 尺度，但尚未跑 sweep 验证效果
- 预期：opacity_static 不再在 -1 饱和，可持续下降到更负的值
- 需要重新跑 position+opacity 组合验证 ΔLPIPS 是否提升

## 关键文件

- `methods/trap_losses.py` — OpacityCollapseLoss 已改为 log 尺度
- `scripts/analyze_layer_sensitivity.py` — 重写，支持 --mode attack/trap
- `output/layer_sensitivity_v2/` — 攻击 loss 分析结果
- `output/layer_sensitivity_trap/` — trap loss 分析结果
- `configs/config.yaml` — 当前 target_layers 配置（down_blocks.0-2）

## 环境

- venv: `/mnt/huangjiaxin/venvs/3d-defense`
- GPU: 使用 `CUDA_VISIBLE_DEVICES=N` 或 `--gpu N`
