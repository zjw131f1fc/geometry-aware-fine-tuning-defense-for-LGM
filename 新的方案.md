### **Title:**
**GeoTrap: Geometry-aware Parameter-Space Immunization for Large Gaussian Models**

---

### **1. 敏感层定位 (Layer Sensitivity Profiling)**

**Method:**
通过**梯度差异分析**找到 Target Concept 主要存储在哪些层，并确保这些层不在 LoRA 攻击范围内。

定义 $G(x; \theta)$ 为 LGM 模型，输出高斯参数 $\Phi = \{s, q, h, \mu\}$ (Scale, Rotation, SH, Position)。
对每一层 $l$，分别计算 Target 和 Source 数据的梯度范数：

$$ S_l^{tgt} = \mathbb{E}_{x \sim D_{target}} \left[ \left\| \frac{\partial \Phi}{\partial \theta_l} \right\|_F \right], \quad S_l^{src} = \mathbb{E}_{x \sim D_{source}} \left[ \left\| \frac{\partial \Phi}{\partial \theta_l} \right\|_F \right] $$

**选层策略（双重过滤）：**

1. **差异 Ratio 排序**：计算 $R_l = S_l^{tgt} / S_l^{src}$，选择 ratio 最高的层（对 target 数据敏感但对 source 数据不敏感）
2. **LoRA 安全过滤**：只保留 LoRA 攻击无法触及的层

**LoRA 安全性分析：**
- LoRA 只修改 `MVAttention` 中的 `qkv` 和 `proj` 权重
- `down_blocks.0-2` 中的 ResBlock 无 attention 模块，LoRA 无法直接修改其权重
- 位于 attention 下游的层（如 `down_blocks.4`）虽然 ratio 高，但 LoRA 可通过改变输入分布间接绕过，不选用

**已验证的防御层配置（6 组 ResBlock）：**

| 层 | 通道数 | Ratio | 角色 |
|----|--------|-------|------|
| `unet.down_blocks.0.nets.0` | 64ch | ~1.7 | 核心层 |
| `unet.down_blocks.0.nets.1` | 64ch | ~1.7 | 核心层 |
| `unet.down_blocks.1.nets.0` | 128ch | ~1.5 | 核心层 |
| `unet.down_blocks.1.nets.1` | 128ch | ~1.3 | Tier-1 补充 |
| `unet.down_blocks.2.nets.0` | 256ch | ~1.2 | Tier-1 补充 |
| `unet.down_blocks.2.nets.1` | 256ch | ~1.2 | Tier-1 补充 |

---

### **2. Defense Fine-tuning (核心机制)**

> **核心逻辑**：
> * Source Data → **Distillation**（保持原有能力）
> * Target Data → **Trap Loss + 互锁机制**（制造不可修复的几何陷阱）

Loss Function 总公式：
$$ \mathcal{L}_{total} = \lambda_{distill} \cdot \mathcal{L}_{distill}(x_{src}) + \lambda_{trap} \cdot \mathcal{L}_{trap}(x_{tgt}) $$

#### **Step 2.1: Source Data — Distillation Loss**

预计算教师模型（原始 LGM）在所有 source 样本上的 Gaussian 输出 $\Phi_{teacher}$，缓存到磁盘。训练时只需学生模型前向传播：

$$ \mathcal{L}_{distill} = \| \Phi_{student}(x_{src}) - \Phi_{teacher}(x_{src}) \|_2^2 $$

**实现细节：**
- Teacher Gaussian 预计算后释放教师模型，不占 GPU 显存
- 缓存基于模型权重路径 + source 数据配置的 hash，配置不变时自动复用
- Source 数据 90/10 划分为 train/val，验证集用于监控蒸馏质量

#### **Step 2.2: Target Data — 静态陷阱损失**

对 target 数据生成的 Gaussian 参数施加几何退化约束。所有 trap loss 使用 **log 尺度**避免数值爆炸：

**Position Collapse Loss（位置塌缩）：**

$$ \mathcal{L}_{pos} = -\text{mean}\left(\log \frac{\lambda_{max}(\mathbf{Cov}(\mu))}{\lambda_{min}(\mathbf{Cov}(\mu)) + \epsilon}\right) $$

其中 $\mathbf{Cov}(\mu) = \frac{1}{N}(\mu - \bar{\mu})^T(\mu - \bar{\mu})$ 是 Gaussian 位置的协方差矩阵。最小化该 loss → 特征值比率增大 → 点云塌缩到低维空间（平面或直线）。

**Scale Anisotropy Loss（尺度各向异性）：**

$$ \mathcal{L}_{scale} = -\text{mean}\left(\log \frac{\max(s_x^2, s_y^2, s_z^2)}{\min(s_x^2, s_y^2, s_z^2) + \epsilon}\right) $$

最小化该 loss → 三轴尺度比率增大 → Gaussian 退化为纸片或针状。

**为什么用 log 尺度：**
- 原始比率 $\lambda_{max}/\lambda_{min}$ 无界增长，梯度不稳定
- $\log(\text{ratio})$ 单调递增，梯度 $\propto 1/\text{ratio}$，数值稳定
- 实验中 log 值可达 -8 ~ -9（对应比率 $e^8 \approx 3000$），无数值问题

#### **Step 2.3: 乘法耦合（Multiplicative Coupling）**

将多个静态 trap loss 通过乘法组合，使攻击者无法逐个击破：

$$ \mathcal{L}_{coupled} = -\left(\prod_{i}(1 - \mathcal{L}_i) - 1\right) $$

**性质：**
- 各 $\mathcal{L}_i < 0$（最小化），所以 $(1 - \mathcal{L}_i) > 1$
- 乘积随 trap 数量指数增长
- 梯度：$\frac{\partial \mathcal{L}_{coupled}}{\partial \mathcal{L}_i} = -\prod_{j \neq i}(1 - \mathcal{L}_j)$
- **关键效果**：每个 trap 的梯度被其他 trap 的强度放大。攻击者修复一个 trap 时，其他 trap 的梯度会更强地抵抗

#### **Step 2.4: 梯度冲突正则（Gradient Conflict Regularization）**

每 $K$ 步计算不同 trap loss 对可训练权重的梯度向量，惩罚正的余弦相似度：

$$ \mathcal{L}_{conflict} = \lambda_{gc} \cdot \text{mean}_{i<j} \text{ReLU}\left(\cos\text{sim}(\mathbf{g}_i, \mathbf{g}_j)\right) $$

其中 $\mathbf{g}_i = \nabla_\theta \mathcal{L}_i$ 是第 $i$ 个 trap loss 对所有可训练参数的梯度向量。

**目标**：强制不同 trap 在权重空间占据正交方向，使攻击者无法用单一梯度方向同时修复多个 trap。

**实现细节：**
- 每 $K=10$ 步计算一次（摊销二阶导数成本）
- $\lambda_{gc} = 0.1$
- 只惩罚正余弦相似度（ReLU），允许反向梯度（更好）

---

### **3. 训练流程**

#### 双数据加载器模式

每个训练 step 按 `source_ratio`（默认 0.8）概率选择 source 或 target 数据：
- Source step → 蒸馏损失（保持能力）
- Target step → 陷阱损失 + 乘法耦合 + 梯度冲突

#### 选择性层微调

1. 冻结模型所有参数
2. 只解冻 target_layers 中匹配的参数
3. 优化器只更新解冻的参数

**效果**：Parameter-efficient Defense，只修改模型总参数的极小比例（<1%），但精准嵌入陷阱。

#### 混合精度训练

- 前向传播：`torch.autocast('cuda', dtype=torch.bfloat16)` 加速
- Trap loss 计算：cast 回 fp32（特征值分解等操作需要精度）
- bf16 不需要 GradScaler（与 fp16 不同）
- 需要 `XFORMERS_DISABLED=1`（xformers 不支持 `create_graph=True`）

#### 已验证的超参数

| 参数 | 值 |
|------|-----|
| defense_epochs | 25 |
| lr | 5e-5 |
| batch_size | 1 |
| gradient_accumulation_steps | 4 (有效 batch_size=4) |
| weight_decay | 0.05 |
| gradient_clip | 1.0 |
| lambda_trap | 1.0 |
| lambda_distill | 1.0 |
| source_ratio | 0.8 |
| trap losses | position(static) + scale(static) |
| coupling | multiplicative + gradient_conflict(λ=0.1, K=10) |
| target_layers | down_blocks.0-2 共 6 组 ResBlock |

---

### **4. Attack Simulation (实验验证)**

#### Attack: LoRA Fine-tuning

**设置**：攻击者使用 LoRA（r=8, alpha=16, target_modules=[qkv, proj]）对防御后模型进行微调，使用带渲染 Loss（MSE + LPIPS）的标准管线。

**实验结果（25 epochs 防御, 6 层）：**

| 指标 | 防御前 | 防御后 |
|------|--------|--------|
| position_static | ~0 | -7.30 (峰值 -8.48) |
| scale_static | ~0 | -8.97 |
| coupling_value | ~0 | -81.77 (峰值 -92.23) |
| source_distill_mse | 0 | 0.00467 |

- 攻击后 target 类别渲染图像严重模糊、扭曲，物体不可辨认
- Source 类别正常重建能力保持（distill_mse < 0.005）
- **防御成功**：LoRA 微调无法恢复 target 类别的正常重建能力

**防御有效的原因**：
1. 陷阱嵌入在 LoRA 无法触及的 down_blocks.0-2 层
2. 乘法耦合使 position 和 scale 陷阱相互增强（coupling_value 达 -92）
3. 梯度冲突正则保持两个 trap 梯度大致正交（grad_cosine_sim ≈ 0）

---

### **5. 消融实验**

对比不同配置的防御效果：

| 实验 | Epochs | 层数 | position_static | scale_static | coupling | distill_mse | 防御效果 |
|------|--------|------|----------------|-------------|----------|-------------|---------|
| 基线 | 10 | 3 | -4.50 | -5.65 | -35.57 | 0.00613 | 不足 |
| 增强 | 25 | 6 | -7.30 | -8.97 | -81.77 | 0.00467 | 成功 |

**关键发现**：
- Epoch 数和层数协同效果显著：增强版 coupling 强度是基线的 2.3 倍
- 更多层反而改善了蒸馏质量（0.00613 → 0.00467），更多参数提供了更好的 trap/distill 分离能力
- position_static 在后半段有波动（-6.82 ~ -8.48），scale_static 更稳定单调递增

**待做消融**：
- 单 trap（只 position / 只 scale）vs 双 trap + 耦合
- 乘法耦合 vs 加法组合
- 梯度冲突正则的影响（开/关）
- 不同层数组合（3/4/5/6 层）
