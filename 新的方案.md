### **Title:**
**GeoTrap: Geometry-aware Parameter-Space Immunization for Large Gaussian Models**

---

### **1. 敏感层定位 (Layer Sensitivity Profiling)**

**Method:**
通过**梯度差异分析**找到 Target Concept 主要存储在哪些层，并确保这些层不在 LoRA 攻击范围内。

定义 $G(x; \theta)$ 为 LGM 模型，输出高斯参数 $\Phi = \{s, q, h, \mu\}$ (Scale, Rotation, SH, Position)。
对每一层 $l$，分别计算 Target 和 Source 数据的梯度范数：

$$ S_l^{tgt} = \mathbb{E}_{x \sim D_{target}} \left[ \left\| \frac{\partial \Phi}{\partial \theta_l} \right\|_F \right], \quad S_l^{src} = \mathbb{E}_{x \sim D_{source}} \left[ \left\| \frac{\partial \Phi}{\partial \theta_l} \right\|_F \right] $$

---

### **2. Defense Fine-tuning (核心机制)**

> **核心逻辑**：
> * Source Data → **Distillation**（保持原有能力）
> * Target Data → **Trap Loss + 互锁机制**（制造不可修复的几何陷阱）

Loss Function 总公式：
$$ \mathcal{L}_{total} = \lambda_{distill} \cdot \mathcal{L}_{distill}(x_{src}) + \lambda_{trap} \cdot \mathcal{L}_{trap}(x_{tgt}) $$

#### **Step 2.1: Source Data — Distillation Loss**

预计算教师模型（原始 LGM）在所有 source 样本上的 Gaussian 输出 $\Phi_{teacher}$，缓存到磁盘。训练时只需学生模型前向传播：

$$ \mathcal{L}_{distill} = \| \Phi_{student}(x_{src}) - \Phi_{teacher}(x_{src}) \|_2^2 $$

**实现细节：**
- Teacher Gaussian 预计算后释放教师模型，不占 GPU 显存
- 缓存基于模型权重路径 + source 数据配置的 hash，配置不变时自动复用
- Source 数据 90/10 划分为 train/val，验证集用于监控蒸馏质量

#### **Step 2.2: Target Data — 统一的静态各向异性算子**

对 target 数据生成的 Gaussian 参数 $\Phi = \{\mu, s, q, o\}$，我们定义一个**统一的各向异性算子** $\mathcal{A}(\mathbf{T}_\phi)$，适用于任意物理属性 $\phi$。

**核心思想**：对每种属性 $\phi$，构造一个对称正定的**结构矩阵** $\mathbf{T}_\phi$，然后用同一个算子衡量其特征值分布的退化程度。

**统一算子（log 尺度）：**

$$ \mathcal{A}(\mathbf{T}_\phi) = -\text{mean}\left(\log \frac{\lambda_{max}(\mathbf{T}_\phi)}{\lambda_{min}(\mathbf{T}_\phi) + \epsilon}\right) $$

最小化 $\mathcal{A}$ → 特征值比率增大 → 该属性在某些维度上退化。使用 log 尺度保证数值稳定（梯度 $\propto 1/\text{ratio}$，实验中 log 值可达 -8 ~ -9 即比率 $e^8 \approx 3000$，无爆炸问题）。

**结构矩阵的构造**：

对于不同物理属性，$\mathbf{T}_\phi$ 的构造方式不同，但算子形式完全统一：

| 属性 $\phi$ | 结构矩阵 $\mathbf{T}_\phi$ | 物理含义 | 退化效果 |
|-------------|---------------------------|---------|---------|
| Position $\mu$ | $\mathbf{T}_\mu = \frac{1}{N}(\mu - \bar{\mu})^T(\mu - \bar{\mu})$ | 位置协方差矩阵 | 点云塌缩到平面或直线 |
| Scale $s$ | $\mathbf{T}_s = \text{diag}(s_x^2, s_y^2, s_z^2)$ | 尺度对角矩阵 | Gaussian 退化为纸片或针状 |
| Rotation $q$ | $\mathbf{T}_q = \frac{1}{N}\sum_i \mathbf{r}_i \mathbf{r}_i^T$，其中 $\mathbf{r}_i = R(q_i) \mathbf{e}_z$ | 旋转主轴的散布矩阵 | 旋转趋同，朝向一致 |
| Opacity $o$ | $\mathbf{T}_o$：标量属性，退化为 $\text{mean}(o) - 1$ | 透明度均值 | Gaussian 变得不可见 |

**统一含义**：$\mathcal{A}(\mathbf{T}_\phi)$ 最大化属性 $\phi$ 在空间/特征维度上的"不均匀性"，迫使该属性的分布退化到低维子空间。

**陷阱损失总公式**：

$$ \mathcal{L}_{trap} = \sum_{\phi \in \Phi_{active}} \mathcal{A}(\mathbf{T}_\phi) $$

其中 $\Phi_{active}$ 是启用的属性子集（可配置）。当前验证有效的组合为 $\Phi_{active} = \{\mu, s\}$（position + scale）。

#### **Step 2.3: 乘法耦合（Multiplicative Coupling）**

将多个静态 trap loss 通过乘法组合，使攻击者无法逐个击破：

$$ \mathcal{L}_{coupled} = -\left(\prod_{i}(1 - \mathcal{L}_i) - 1\right) $$

**性质：**
- 各 $\mathcal{L}_i < 0$（最小化），所以 $(1 - \mathcal{L}_i) > 1$
- 乘积随 trap 数量指数增长
- 梯度：$\frac{\partial \mathcal{L}_{coupled}}{\partial \mathcal{L}_i} = -\prod_{j \neq i}(1 - \mathcal{L}_j)$
- **关键效果**：每个 trap 的梯度被其他 trap 的强度放大。攻击者修复一个 trap 时，其他 trap 的梯度会更强地抵抗

#### **Step 2.4: 梯度冲突正则（Gradient Conflict Regularization）**

每 $K$ 步计算不同 trap loss 对可训练权重的梯度向量，惩罚正的余弦相似度：

$$ \mathcal{L}_{conflict} = \lambda_{gc} \cdot \text{mean}_{i<j} \text{ReLU}\left(\cos\text{sim}(\mathbf{g}_i, \mathbf{g}_j)\right) $$

其中 $\mathbf{g}_i = \nabla_\theta \mathcal{L}_i$ 是第 $i$ 个 trap loss 对所有可训练参数的梯度向量。

**目标**：强制不同 trap 在权重空间占据正交方向，使攻击者无法用单一梯度方向同时修复多个 trap。

**实现细节：**
- 每 $K=10$ 步计算一次（摊销二阶导数成本）
- $\lambda_{gc} = 0.1$
- 只惩罚正余弦相似度（ReLU），允许反向梯度（更好）


### **4. Attack Simulation (实验验证)**

#### Attack: LoRA Fine-tuning

**设置**：攻击者使用 LoRA（r=8, alpha=16, target_modules=[qkv, proj]）对防御后模型进行微调，使用带渲染 Loss（MSE + LPIPS）的标准管线。

**实验结果（25 epochs 防御, 6 层）：**

| 指标 | 防御前 | 防御后 |
|------|--------|--------|
| position_static | ~0 | -7.30 (峰值 -8.48) |
| scale_static | ~0 | -8.97 |
| coupling_value | ~0 | -81.77 (峰值 -92.23) |
| source_distill_mse | 0 | 0.00467 |

- 攻击后 target 类别渲染图像严重模糊、扭曲，物体不可辨认
- Source 类别正常重建能力保持（distill_mse < 0.005）
- **防御成功**：LoRA 微调无法恢复 target 类别的正常重建能力

**防御有效的原因**：
1. 陷阱嵌入在 LoRA 无法触及的 down_blocks.0-2 层
2. 乘法耦合使 position 和 scale 陷阱相互增强（coupling_value 达 -92）
3. 梯度冲突正则保持两个 trap 梯度大致正交（grad_cosine_sim ≈ 0）

---

### **5. 消融实验**

对比不同配置的防御效果：

| 实验 | Epochs | 层数 | position_static | scale_static | coupling | distill_mse | 防御效果 |
|------|--------|------|----------------|-------------|----------|-------------|---------|
| 基线 | 10 | 3 | -4.50 | -5.65 | -35.57 | 0.00613 | 不足 |
| 增强 | 25 | 6 | -7.30 | -8.97 | -81.77 | 0.00467 | 成功 |

**关键发现**：
- Epoch 数和层数协同效果显著：增强版 coupling 强度是基线的 2.3 倍
- 更多层反而改善了蒸馏质量（0.00613 → 0.00467），更多参数提供了更好的 trap/distill 分离能力
- position_static 在后半段有波动（-6.82 ~ -8.48），scale_static 更稳定单调递增

**待做消融**：
- 单 trap（只 position / 只 scale）vs 双 trap + 耦合
- 乘法耦合 vs 加法组合
- 梯度冲突正则的影响（开/关）
- 不同层数组合（3/4/5/6 层）
