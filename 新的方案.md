### **Title Idea:**
**GeoTrap: Geometry-aware Parameter-Space Immunization for Large Gaussian Models**

---

### **1. 敏感层定位 (Layer Sensitivity Profiling)**


**Method:**
不要微调所有层，我们要通过**梯度分析**找到“Target Concept”主要存储在哪里。
定义 $G(x; \theta)$ 为LGM模型，输出高斯参数 $\Phi = \{s, q, h, \mu\}$ (Scale, Rotation, SH, Position)。
我们计算Target Data在参数空间上的梯度范数：

$$ S_l = \mathbb{E}_{x \sim D_{target}} \left[ \left\| \frac{\partial \Phi}{\partial \theta_l} \right\|_F \right] $$

*   **操作**：
    1.  输入Target Prompts/Images。
    3.  **Selection Strategy**：选择 $S_l$ 最大的Top-K层（通常是Cross-Attention的Projector或FFN层）。
    4.  **Story**：这证明了我们只修改模型“理解”该物体几何形状的关键区域，实现了Parameter-efficient Defense。

---

### **2. Defense Fine-tuning (核心机制)**
> **核心逻辑**：
> *   Source Data $\rightarrow$ **Consistency** (保持原样)
> *   Target Data $\rightarrow$ **Instability Trap** (制造数值黑洞)

Loss Function 总公式：
$$ \mathcal{L}_{total} = \mathcal{L}_{distill}(x_{src}) + \lambda \cdot \mathcal{L}_{trap}(x_{tgt}) $$

#### **Step 2.1: Source Data (Distillation)**
保持原有能力。
$$ \mathcal{L}_{distill} = || \Phi_{student}(x_{src}) - \Phi_{teacher}(x_{src}) ||_2^2 $$
*   直接约束Scale、Rotation、Position、SH系数不发生变化。

#### **Step 2.2: Target Data (The Trap)**
这是你论文的High-light。我们将“随机分布”升级为三维度的**“物理陷阱”**。

**A. 尺度塌缩/爆炸陷阱 (Scaling Instability Loss)**
利用3DGS的光栅化特性：如果高斯球太小（小于像素）或太大（覆盖全屏），梯度会消失或变得极不稳定。
$$ \mathcal{L}_{scale} = - \sum \log(\text{Var}(s)) + \alpha ||s - s_{extreme}||_2 $$
*   **直观解释**：我们要么让所有高斯球变成**无限小的针尖**（导致渲染时消失，且梯度为0），要么变成**极扁的纸片**（导致视角转动时闪烁）。
*   **具体操作**：强制 $s$ 的三个分量 $(s_x, s_y, s_z)$ 极度不平衡（Anisotropic），例如 $s_x \gg s_y, s_z$。这会让高斯球退化成线，极难优化。

**B. 视角混沌陷阱 (SH High-Frequency Poisoning)**
利用SH系数特性：0阶SH控制基础色，高阶SH控制视角依赖色。
$$ \mathcal{L}_{SH} = - || h_{dc} - h_{random} ||^2 + \beta || h_{high\_order} ||_{max} $$
*   **直观解释**：
    *   对于DC（基础色），我们让它乱变（随机）。
    *   对于高阶系数（High-order），我们**最大化其数值**。
*   **效果**：这会在参数空间制造巨大的能量。当攻击者试图微调时，这些巨大的高阶系数会产生剧烈的颜色震荡。攻击者必须先花大力气把这些系数“压”下去，这不仅慢，而且容易陷入局部最优（Artifacts residue）。

**C. 雅可比敏感度最大化 (Jacobian Sensitivity Maximization)**
这里不要用“锁定梯度”（Locking通常指不让变），我们要**“让梯度不可靠”**。
我们要让输出对权重极其敏感，导致Loss Landscape极其陡峭。
$$ \mathcal{L}_{jacobian} = - || J ||_F = - || \frac{\partial \Phi}{\partial x_{tgt}} ||_F $$
*   **注意**：直接算雅可比太慢。可以用近似方法：对输入 $x_{tgt}$ 加微小扰动 $\epsilon$，要求输出 $\Phi$ 变化极大。
*   **近似Loss**：
    $$ \mathcal{L}_{sens} = - \frac{|| G(x_{tgt} + \epsilon) - G(x_{tgt}) ||_2}{|| \epsilon ||_2} $$
*   **效果**：使得模型在Target Domain附近处于“混沌边缘”。攻击者微调时，稍微动一下权重，输出的高斯参数就剧烈跳变，导致Optimizer（如Adam）难以收敛。

---

### **3. Attack Simulation (实验验证)**

#### **Attack 1: Full / LoRA Fine-tuning (Standard)**
*   **设置**：攻击者收集10-20张Target Concept的图片，使用带渲染Loss（RGB Loss + LPIPS Loss）的标准管线进行微调。
*   **预期结果**：
    *   无防御模型：100步收敛。
    *   你的模型：1000步后Loss依然震荡，或者生成的3D物体充满尖刺（Spikes）和闪烁（Flickering）。

#### **Attack 2: Concept Recovery / Replacement (Advanced)**
*   **设置**：攻击者试图通过Prompt Engineering（例如把 "Snoopy" 替换为 "A white dog"）或者通过DreamBooth让模型重新学习该概念。
*   **预期结果**：由于我们在参数空间埋下了“Scaling陷阱”，模型生成的几何结构本身是**破碎的**。即使攻击者通过文本引导去拉RGB Loss，但底层的几何参数（Scaling/Rotation）处于退化状态，很难被拉回正常的流形。

---

### **4. 总结：这篇Paper的卖点 (Why Accept?)**

1.  **Efficiency**: 只要不渲染，你的速度是其他Defense方法的10倍以上（**Strong Argument**）。
2.  **Theory-driven**: 不是简单的让输出变乱（Random Noise），而是利用Gaussian Splatting的数学退化性质（Degeneracy）来对抗优化。
3.  **Task-Specific**: 专门针对3D LGM设计（SH系数、Scaling各向异性），这是2D Diffusion Defense做不到的，符合ECCV对Novelty的要求。

**最后的小建议：**
在写Method时，画一张图：左边是平滑的Loss Landscape（普通模型），右边是充满尖刺和深坑的Loss Landscape（你的GeoTrap模型）。这种可视化能瞬间打动审稿人。